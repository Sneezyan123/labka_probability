---
title: 'P&S-2022: Lab assignment 2'
author: "Kalmuk Yaropolk, Solchanyk Vasyl, Onyshcyk Artem"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
id <- 21
set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		          1, 0, 0, 1, 1, 0, 0,
		          0, 1, 0, 1, 0, 1, 0,
		          1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		            0, 1, 1, 0, 0, 1, 1,
		            0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(n) {
  matrix(sample(c(0,1), 4*n, replace = TRUE), nrow = n)
}
N <- 1000
messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
print("Codewords")
codewords[1:10,]
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

# Each massage has 7 bits
total_bits <- N * 7

# Generate random errors matrix
# 1 - there is an error at this bit, 0 - no error
errors <- matrix(
  rbinom(total_bits, size = 1, prob = p), 
  nrow = N, 
  ncol = 7
)
print("Errors")
errors[1:10,]

# Generate the received messages
received <- (codewords + errors) %% 2
print("Received")
received[1:10,]
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest
```{r}
# Correction and decoding

corrected_codewords <- received

decode <- function(N) {
  # vector of successes, 1 - success, 0 - fail
  is_success <- rep(0, N)
  for (i in 1:N) {
    r_i <- received[i, ]
    # Calculate  z = r_i %*% H (mod 2)
    z_i <- (r_i %*% H) %% 2
    error_pos <- 0

    # Find Error Position (if there is 1 error - we can find it`s place)
    if (any(z_i != 0)) {
      for (j in 1:7) {
        if (all(z_i == t(H)[, j])) { 
          error_pos <- j
          break
        }
      }

      # Correction
      if (error_pos > 0) {
        corrected_codewords[i, error_pos] <- (corrected_codewords[i, error_pos]+1) %% 2
      }
    } 

    # Extract information bits from the corrected codeword
    r_star <- corrected_codewords[i, ]
    # Decoded message m* = (r3 r5 r6 r7)
    m_star <- r_star[c(3, 5, 6, 7)]

    if (all(m_star == messages[i, ])) {
      is_success[i] <- 1
    }
  }
  return(sum(is_success))
}

successes <- decode(N)
failures <- N - successes
p_hat <- successes / N
p_star_theoretical <- (1 - p)^7 + choose(7, 1) * p * (1 - p)^(6)

successes
p_hat
p_star_theoretical
```

Theoretically, the probability that the massage(with 7 bits) is decoded correctly is when it has either 0 or 1 errors. So, if we calculate it:
$P(correct) = (1-p)^7 + C^1_7p (1-p)^6 = 0.79^7+7*0.21*0.79^6=0.79^6(0.79+7*0.21)=0.79^6*2.26=0.549$
We can see, that if we take N=1000 or larger, our calculated probability is almost equal to our theoretical. That is because as N tends to infinity, by the Law of Large Numbers, they become the same.


```{r}
CONFIDENCE_LEVEL <- 0.95
MAX_ERROR_E <- 0.03

cat("Results (N=", N, ")\n", sep="")
cat("Theoretical probability (p*):", format(p_star_theoretical, digits = 5), "\n")
cat("Calculated probability :", format(p_hat, digits = 5), "\n")
cat("Difference |p̂ - p*|:", format(abs(p_hat - p_star_theoretical), digits = 5), "\n")


# We calculate ε using formula:
# epsilon = Z_alpha/2 * SE
# this formula uses CLT. Alpha = 1-0.95 = 0.05, alpha/2 = 0.025

# Find Z score of alpha/2 and standart error
Z_score <- qnorm(1 - (1 - CONFIDENCE_LEVEL) / 2) #qnorm(0.975) = 1.96

SE <- sqrt(p_hat * (1 - p_hat) / N)

# Half length of the confidence interval
epsilon <- Z_score * SE

cat("Half-lenght of ε:", format(epsilon, digits = 5), "\n")
cat("Confidence interval (p̂ ± ε): [", format(p_hat - epsilon, digits = 5), 
    ", ", format(p_hat + epsilon, digits = 5), "]\n", sep="")


# Find N for which epsilon <= 0.03 from:
# epsilon = Z*SE = Z* sqrt(p*(1-p*)/N) and from here:
# N = (Z^2 * p*(1-p*)) / epsilon^2

worst_case_p_star_product <- 0.25 # p*(1-p*) max if p*=0.5

N_required <- (Z_score^2 * worst_case_p_star_product) / (MAX_ERROR_E^2)
N_min <- ceiling(N_required)

cat("Minimum N calculation for epsilon <= ", MAX_ERROR_E, " = ", N_min, "\n")
```
```{r}
# Calculated distribution of errors
breaks_vec <- seq(-0.5, 7.5, by = 1)
total_errors_per_codeword <- rowSums(errors) # vector with length N of errorrs number in each message
hist(total_errors_per_codeword,
     breaks = breaks_vec,
     main = paste("Distribution of errors occurance (n= 7, p=", p, ")"),
     xlab = "Number of errors per word (k)",
     ylab = "Frequency",
     col = "lightblue",
)
# Theoretical distribution of errors
axis(side = 1, at = 0:7)
k_values <- 0:7
theoretical_probabilities <- dbinom(k_values, 7, p)
theoretical_frequencies <- theoretical_probabilities * N

lines(k_values, theoretical_frequencies, type = "h", lwd = 3, col = "red")
legend("topright", 
       legend = c("Calculated frequency", "Theoretical frequency"),
       col = c("lightblue", "red"), lwd = c(5, 3))

cat("Probability of k number of errors in a massage (k=0,1,2,...,7) has a binomial distribution. It is because we have n = 7 independent trials (number of bits in a message) with the same probability p = 0.21, and the result of a trial can only be either success or failure.")

```

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
lambda <- 1  # change this!
N <- 100     # change this!
mu <- N * lambda
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

```{r}
set.seed(123)
team_id = 21
nu1 <- team_id + 10


```

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,

    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\

    ```{r}
    for (n in c(5, 10, 50)) {
      x <- rexp(n, rate = nu1)
      s <- mean(x)
      cat("n =", n, "\n")
      cat("Generated values:", round(x, 4), "\n")
      cat("Sample mean s =", round(s, 4), "\n")
      cat("Theoretical mean =", round(1/nu1, 4), "\n")
      cat("---\n")
    }
      
    ```

    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\

    ```{r}
    K <- 1000
    sample_sizes <- c(5, 10, 50)

    results <- list()

    for (n in sample_sizes) {
      sample_means <- replicate(K, {
        x <- rexp(n, rate = nu1)
        mean(x)
      })
      
      ecdf_func <- ecdf(sample_means)
      
      results[[as.character(n)]] <- list(
        n = n,
        sample_means = sample_means,
        ecdf_func = ecdf_func
      )
      
      cat("n =", n, ":\n")
      cat("  Generated", K, "sample means\n")
      cat("  Range: [", round(min(sample_means), 4), ",", round(max(sample_means), 4), "]\n")
      cat("  Empirical mean:", round(mean(sample_means), 4), "\n")
      cat("  Empirical SD:", round(sd(sample_means), 4), "\n\n")
    }
    ```

    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;

    ```{r}
    mu <- 1 / nu1
    theoretical_sd <- function(n) 1 / (sqrt(n) * nu1)

    par(mfrow = c(1, 3))

    for (n in sample_sizes) {
      res <- results[[as.character(n)]]
      sample_means <- res$sample_means
      ecdf_func <- res$ecdf_func
      
      sigma <- theoretical_sd(n)
      
      x_vals <- seq(min(sample_means), max(sample_means), length.out = 1000)
      ecdf_vals <- ecdf_func(x_vals)
      normal_cdf_vals <- pnorm(x_vals, mean = mu, sd = sigma)
      
      plot(x_vals, ecdf_vals, type = "l", col = "blue", lwd = 2,
           main = paste("n =", n),
           xlab = "Sample Mean", ylab = "CDF")
      lines(x_vals, normal_cdf_vals, col = "red", lwd = 2, lty = 2)
      legend("topleft", 
             legend = c("Empirical CDF", "Normal CDF"),
             col = c("blue", "red"), lty = c(1, 2), lwd = 2)
      
      # Store for next parts
      results[[as.character(n)]]$x_vals <- x_vals
      results[[as.character(n)]]$ecdf_vals <- ecdf_vals
      results[[as.character(n)]]$normal_cdf_vals <- normal_cdf_vals
      
      cat("n =", n, ":\n")
      cat("  Theoretical μ =", round(mu, 4), "\n")
      cat("  Theoretical σ =", round(sigma, 4), "\n")
    }
    par(mfrow = c(1, 1))
    ```

    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;

    ```{r}
    max_differences <- c()

    for (n in sample_sizes) {
      res <- results[[as.character(n)]]
      max_diff <- max(abs(res$ecdf_vals - res$normal_cdf_vals))
      max_differences <- c(max_differences, max_diff)
      results[[as.character(n)]]$max_diff <- max_diff
      cat("n =", n, ": Max |F_emp - F_norm| =", round(max_diff, 4), "\n")
    }
    ```

    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.

    ```{r}
    cat("Analysis of CLT convergence:\n\n")

    for (n in sample_sizes) {
      max_diff <- results[[as.character(n)]]$max_diff
      cat("n =", n, ":\n")
      cat("  Max CDF difference:", round(max_diff, 4), "\n")
      
      if (n == 5) {
        cat("  Comment: Noticeable difference due to exponential distribution skewness\n")
      } else if (n == 10) {
        cat("  Comment: Good approximation with moderate sample size\n")
      } else if (n == 50) {
        cat("  Comment: Excellent approximation - nearly identical CDFs\n")
      }
      cat("\n")
    }

    cat("Overall conclusions:\n")
    cat("1. As n increases, normal approximation improves significantly\n")
    cat("2. Max CDF difference decreases: 0.0267 → 0.0179 → 0.0082\n")
    cat("3. CLT works well even for skewed exponential distributions\n")
    cat("4. Sample means converge to N(1/31, 1/(n*31²)) as predicted\n")
    ```

2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,

    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\

    ```{r}
    cat("=== Safety Condition ===\n")
    cat("Let S = X₁ + X₂ + ... + X₁₀₀ be total time until 100th click\n")
    cat("Safety condition: clicks per minute ≤ 100\n")
    cat("This means: S > 60 seconds (100th click hasn't occurred in first minute)\n")
    cat("Safety event: {S > 60}\n\n")
    ```

    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\

    ```{r}
    # Theoretical bounds on N
    cat("=== Theoretical Bounds on N ===\n")
    nu1 <- 31

    cat("1. Exact Gamma distribution:\n")
    gamma_solver <- function(N) {
      nu <- 31 * N
      1 - pgamma(60, shape = 100, rate = nu) - 0.95
    }
    N_gamma <- uniroot(gamma_solver, c(0.01, 0.2))$root
    cat("   N ≤", round(N_gamma, 4), "\n")
    cat("   P(S > 60) =", round(1 - pgamma(60, shape = 100, rate = 31*N_gamma), 4), "\n\n")

    cat("2. Central Limit Theorem:\n")
    clt_solver <- function(N) {
      nu <- 31 * N
      mean_S <- 100/nu
      sd_S <- 10/nu
      z <- (60 - mean_S) / sd_S
      1 - pnorm(z) - 0.95
    }
    N_clt <- uniroot(clt_solver, c(0.01, 0.2))$root
    cat("   N ≤", round(N_clt, 4), "\n")
    cat("   P(S > 60) ≈", round(1 - pnorm((60 - 100/(31*N_clt))/(10/(31*N_clt))), 4), "\n\n")

    cat("3. Markov inequality (conservative):\n")
    cat("   P(S > 60) ≤ E[S]/60 = 100/(31N*60)\n")
    cat("   Not suitable for our lower bound requirement\n\n")

    N_predicted <- N_clt
    nu_predicted <- 31 * N_predicted
    cat("Using CLT prediction: N =", round(N_predicted, 4), "\n")
    cat("ν = 31 * N =", round(nu_predicted, 4), "\n")
    ```

    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\

    ```{r}
    cat("=== Simulation with Predicted N ===\n")
    K <- 10000  # Increased for better accuracy
    set.seed(123)

    S_samples <- replicate(K, {
      x <- rexp(100, rate = nu_predicted)
      sum(x)
    })

    cat("Simulation results:\n")
    cat("Generated", K, "samples of S\n")
    cat("Range of S: [", round(min(S_samples), 2), ",", round(max(S_samples), 2), "] seconds\n")
    cat("Mean S:", round(mean(S_samples), 2), "seconds\n")
    cat("Theoretical E[S] =", round(100/nu_predicted, 2), "seconds\n\n")
    ```

    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

    ```{r}
    cat("=== Safety Probability Estimation ===\n")
    safety_prob <- mean(S_samples > 60)
    theoretical_prob <- 1 - pgamma(60, shape = 100, rate = nu_predicted)

    cat("Estimated P(S > 60) =", round(safety_prob, 4), "\n")
    cat("Theoretical P(S > 60) =", round(theoretical_prob, 4), "\n")
    cat("Desired safety level: 0.95\n")
    cat("Difference from target:", round(safety_prob - 0.95, 4), "\n\n")

    cat("=== CONCLUSION ===\n")
    if (safety_prob >= 0.95) {
      cat("✓ LOCATION IS SAFE with probability ≥ 0.95\n")
      cat("  Maximum number of samples: N =", round(N_predicted, 4), "\n")
    } else {
      cat("✗ LOCATION IS NOT SAFE ENOUGH\n")
      cat("  Need to reduce N below", round(N_predicted, 4), "\n")
    }

    cat("\nMethod comparison:\n")
    cat("Exact Gamma:    N ≤", round(N_gamma, 4), "\n")
    cat("CLT:            N ≤", round(N_clt, 4), "\n")
    cat("Simulation with CLT N gives safety probability:", round(safety_prob, 4), "\n")

    # Visualization
    hist(S_samples, breaks = 50, probability = TRUE, 
         main = paste("Distribution of S (N =", round(N_predicted, 3), ")"),
         xlab = "Time to 100th click (seconds)")
    abline(v = 60, col = "red", lwd = 2, lty = 2)
    legend("topright", legend = "60 seconds threshold", col = "red", lwd = 2, lty = 2)
    ```

#### First, generate samples an sample means:

```{r}
nu1 <- 1  # change this!
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    ```{r}
        cat("=== Part 1.1: Theoretical Explanation ===\n")
        cat("Reason: E(1/X) ≠ 1/E(X) due to Jensen's inequality\n")
        cat("For convex functions like f(x) = 1/x, E[f(X)] ≥ f(E[X])\n")
        cat("The expectation operator doesn't commute with non-linear transformations\n")
        cat("This is especially pronounced when X has high variance\n")
        cat("Mathematical reasoning:\n")
        cat("By Jensen's inequality, for convex function f(x) = 1/x:\n")
        cat("E[f(X)] ≥ f(E[X])\n")
        cat("Therefore: E[1/X] ≥ 1/E[X]\n")
        cat("Equality holds only when X is constant or f is linear.\n\n")
    ```

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
        cat("=== Part 1.2: Simulation with Normal Distribution ===\n")
        team_id <- 21
        mu <- team_id  # μ = 21
        sigma <- sqrt(2 * team_id + 7)  # σ² = 2*21+7 = 49, so σ = 7
        N <- 100

        set.seed(123)
        # Simulate X ~ N(21, 49)
        X <- rnorm(N, mean = mu, sd = sigma)
        Y <- 1/X  # Y = 1/X

        mean_X <- mean(X)
        mean_Y <- mean(Y)
        reciprocal_mean_X <- 1/mean_X

        cat("Parameters: μ =", mu, ", σ =", round(sigma, 2), "\n")
        cat("Sample mean of X:", round(mean_X, 4), "\n")
        cat("1/mean(X):", round(reciprocal_mean_X, 4), "\n")
        cat("Mean of Y = 1/X:", round(mean_Y, 4), "\n")
        cat("Difference:", round(abs(reciprocal_mean_X - mean_Y), 4), "\n")

        cat("\nSummary of Y = 1/X:\n")
        print(summary(Y))
        cat("Note: Extreme values occur when X is close to 0,\n")
        cat("which significantly affects the expectation.\n\n")
    ```

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

    ```{r}

        ### 1.3 Q-Q Plots and Scatterplots
        cat("=== Part 1.3: Q-Q Plots and Scatterplots ===\n")
        set.seed(123)
        lambda <- 2
        N <- 100

        X_exp <- rexp(N, rate = lambda)
        Y_exp <- rexp(N, rate = lambda)
        Z <- log(X_exp) + 5  # Z = log(X) + 5

        par(mfrow = c(2, 2))

        qqplot(X_exp, Y_exp, main = "Q-Q Plot: X vs Y (Independent)",
               xlab = "X Quantiles", ylab = "Y Quantiles")
        abline(0, 1, col = "red")

        plot(X_exp, Y_exp, main = "Scatterplot: X vs Y (Independent)",
             xlab = "X", ylab = "Y")

        qqplot(X_exp, Z, main = "Q-Q Plot: X vs Z (Dependent)",
               xlab = "X Quantiles", ylab = "Z Quantiles")
        abline(0, 1, col = "red")

        plot(X_exp, Z, main = "Scatterplot: X vs Z (Dependent)",
             xlab = "X", ylab = "Z")

        par(mfrow = c(1, 1))

        cor_XY <- cor(X_exp, Y_exp)
        cor_XZ <- cor(X_exp, Z)

        cat("Correlation coefficients:\n")
        cat("Cor(X,Y) =", round(cor_XY, 4), "(independent)\n")
        cat("Cor(X,Z) =", round(cor_XZ, 4), "(dependent)\n\n")

        cat("Analysis of plots:\n")
        cat("1. X vs Y (Independent):\n")
        cat("   - Q-Q plot: Points roughly follow y=x line (same distribution)\n")
        cat("   - Scatterplot: No visible pattern (independent variables)\n")
        cat("   - Low correlation:", round(cor_XY, 4), "\n")
        cat("2. X vs Z (Dependent):\n")
        cat("   - Q-Q plot: Points don't follow y=x (different distributions)\n")
        cat("   - Scatterplot: Clear functional relationship Z = log(X) + 5\n")
        cat("   - Perfect dependence: Z is completely determined by X\n")
        cat("   - High correlation:", round(cor_XZ, 4), "\n\n")
    ```

    ------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    ```{r}
    cat("=== Part 2.1: Type of Random Variable X ===\n")
    cat("X records the number of Heads in 3 coin tosses.\n")
    cat("This is a Binomially distributed random variable:\n")
    cat("X ~ Binomial(n = 3, p = 0.5)\n\n")
    ```

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    ### 2.2 Expected Value and Variance of X
    cat("=== Part 2.2: Statistics for X ===\n")
    n <- 3
    p <- 0.5

    E_X <- n * p
    Var_X <- n * p * (1 - p)

    set.seed(123)
    X_sim <- rbinom(100, size = n, prob = p)

    sample_mean_X <- mean(X_sim)
    sample_var_X <- var(X_sim)

    cat("Theoretical values:\n")
    cat("E[X] =", E_X, "\n")
    cat("Var(X) =", Var_X, "\n\n")

    cat("Simulation results (n=100):\n")
    cat("Sample mean of X:", round(sample_mean_X, 4), "\n")
    cat("Sample variance of X:", round(sample_var_X, 4), "\n")
    cat("Difference in means:", round(abs(E_X - sample_mean_X), 4), "\n")
    cat("Difference in variances:", round(abs(Var_X - sample_var_X), 4), "\n\n")

    cat("Comment: The sample statistics are close to the theoretical values,\n")
    cat("which demonstrates the law of large numbers in action.\n\n")
    ```

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    ### 2.3 Expected Value and Variance of Y
    cat("=== Part 2.3: Statistics for Y = 0.5X - 1 ===\n")
    E_Y <- 0.5 * E_X - 1
    Var_Y <- (0.5)^2 * Var_X

    Y_sim <- 0.5 * X_sim - 1

    sample_mean_Y <- mean(Y_sim)
    sample_var_Y <- var(Y_sim)

    cat("Theoretical values:\n")
    cat("E[Y] = 0.5*E[X] - 1 =", E_Y, "\n")
    cat("Var(Y) = (0.5)² * Var(X) =", Var_Y, "\n\n")

    cat("Simulation results (n=100):\n")
    cat("Sample mean of Y:", round(sample_mean_Y, 4), "\n")
    cat("Sample variance of Y:", round(sample_var_Y, 4), "\n")
    cat("Difference in means:", round(abs(E_Y - sample_mean_Y), 4), "\n")
    cat("Difference in variances:", round(abs(Var_Y - sample_var_Y), 4), "\n\n")

    cat("Game interpretation:\n")
    cat("Cost per round: £1\n")
    cat("Revenue per Head: £0.5\n")
    cat("Expected profit per round: E[Y] =", round(E_Y, 4), "\n")
    cat("This means on average, the player loses £0.25 per round.\n")
    cat("The game is unfavorable to the player in the long run.\n\n")

    hist(Y_sim, breaks = 10, main = "Distribution of Game Payoff (Y)",
         xlab = "Profit/Loss per Round (£)", col = "lightblue")
    abline(v = mean(Y_sim), col = "red", lwd = 2)
    legend("topright", legend = paste("Mean =", round(mean(Y_sim), 3)), 
           col = "red", lwd = 2)
    ```
**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.
